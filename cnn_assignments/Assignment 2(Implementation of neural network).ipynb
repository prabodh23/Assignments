{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of a neural network in Numpy.\"\"\"\n",
    "\n",
    "from numpy import exp, array, random, dot\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synaptic_weights_1:  [[-0.16595599  0.44064899 -0.99977125]\n",
      " [-0.39533485 -0.70648822 -0.81532281]\n",
      " [-0.62747958 -0.30887855 -0.20646505]]\n",
      "synaptic_weights_2:  [[ 0.07763347]\n",
      " [-0.16161097]\n",
      " [ 0.370439  ]]\n",
      "Error: 0.5108456683181674\n",
      "Error: 0.08985345097113921\n",
      "Error: 0.05505185799884556\n",
      "Error: 0.04376165156296516\n",
      "Error: 0.03766239973729261\n",
      "Error: 0.03369131258706927\n",
      "Error: 0.030836057486838417\n",
      "Error: 0.02865189991455904\n",
      "Error: 0.026908683435973683\n",
      "Error: 0.025473711107384958\n",
      "Error: 0.024264349524812035\n",
      "Error: 0.02322607554826385\n",
      "Error: 0.022321253379121372\n",
      "Error: 0.021522946556508108\n",
      "Error: 0.02081129523372085\n",
      "Error: 0.02017128923412953\n",
      "Error: 0.019591342509985045\n",
      "Error: 0.019062348852520804\n",
      "Error: 0.01857703780527875\n",
      "Error: 0.018129524065358883\n",
      "Error: 0.017714985184049428\n",
      "Error: 0.017329426486602205\n",
      "Error: 0.016969506604944014\n",
      "Error: 0.016632405968075862\n",
      "Error: 0.016315726277890283\n",
      "Error: 0.016017412692213365\n",
      "Error: 0.015735692889535226\n",
      "Error: 0.015469028849866027\n",
      "Error: 0.015216078329390063\n",
      "Error: 0.014975663806604655\n",
      "Error: 0.014746747245702723\n",
      "Error: 0.014528409431800473\n",
      "Error: 0.014319832930533481\n",
      "Error: 0.014120287944158687\n",
      "Error: 0.013929120499924905\n",
      "Error: 0.013745742529612403\n",
      "Error: 0.013569623492671233\n",
      "Error: 0.013400283267054081\n",
      "Error: 0.013237286087204992\n",
      "Error: 0.013080235351760488\n",
      "Error: 0.012928769157317664\n",
      "Error: 0.01278255644130219\n",
      "Error: 0.012641293638172428\n",
      "Error: 0.012504701770142762\n",
      "Error: 0.012372523907239513\n",
      "Error: 0.012244522942521143\n",
      "Error: 0.0121204796372501\n",
      "Error: 0.012000190898120407\n",
      "Error: 0.011883468254643747\n",
      "Error: 0.011770136509750197\n",
      "Error: 0.01166003254075285\n",
      "Error: 0.01155300423123265\n",
      "Error: 0.011448909517241854\n",
      "Error: 0.011347615533605502\n",
      "Error: 0.011248997848100654\n",
      "Error: 0.011152939772983702\n",
      "Error: 0.011059331744764431\n",
      "Error: 0.010968070764338188\n",
      "Error: 0.010879059890623488\n",
      "Error: 0.010792207781732583\n",
      "Error: 0.010707428278460981\n",
      "Error: 0.010624640025531757\n",
      "Error: 0.010543766126587664\n",
      "Error: 0.010464733829410067\n",
      "Error: 0.010387474238260707\n",
      "Error: 0.010311922050604995\n",
      "Error: 0.010238015315790936\n",
      "Error: 0.01016569521353464\n",
      "Error: 0.0100949058502998\n",
      "Error: 0.010025594071874342\n",
      "Error: 0.009957709290625543\n",
      "Error: 0.009891203326081758\n",
      "Error: 0.009826030257630623\n",
      "Error: 0.009762146288245914\n",
      "Error: 0.009699509618273\n",
      "Error: 0.00963808032839423\n",
      "Error: 0.009577820270988833\n",
      "Error: 0.009518692969174444\n",
      "Error: 0.009460663522891006\n",
      "Error: 0.009403698521445616\n",
      "Error: 0.009347765961993177\n",
      "Error: 0.00929283517347742\n",
      "Error: 0.0092388767455998\n",
      "Error: 0.009185862462422745\n",
      "Error: 0.009133765240250249\n",
      "Error: 0.009082559069460224\n",
      "Error: 0.009032218959990623\n",
      "Error: 0.008982720890208278\n",
      "Error: 0.008934041758911543\n",
      "Error: 0.008886159340241064\n",
      "Error: 0.00883905224128801\n",
      "Error: 0.008792699862210637\n",
      "Error: 0.008747082358682929\n",
      "Error: 0.008702180606513763\n",
      "Error: 0.008657976168288825\n",
      "Error: 0.008614451261897665\n",
      "Error: 0.008571588730820656\n",
      "Error: 0.008529372016059317\n",
      "Error: 0.008487785129601685\n",
      "Error: 0.008446812629324821\n",
      "synaptic_weights_1:  [[ -7.1218903   22.10844252 -25.24809957]\n",
      " [ -0.17766852   0.79046298   2.95891129]\n",
      " [ -0.6121298    3.68584963   5.55032188]]\n",
      "synaptic_weights_2:  [[-2.70795811]\n",
      " [ 4.69187501]\n",
      " [-8.58573948]]\n",
      "Output:\n",
      "[[0.00719319]\n",
      " [0.99090447]\n",
      " [0.99090274]\n",
      " [0.00824138]]\n",
      "(3,)\n",
      "[0.0625054]\n"
     ]
    }
   ],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if deriv: return x *(1 - x)\n",
    "    return 1 / (1 + exp(-x))\n",
    "    \n",
    "x = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "y = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights_1 = 2 * random.random((3, 3)) - 1\n",
    "synaptic_weights_2 = 2 * random.random((3, 1)) - 1\n",
    "print(\"synaptic_weights_1: \", synaptic_weights_1)\n",
    "print(\"synaptic_weights_2: \", synaptic_weights_2)\n",
    "\n",
    "for i in range(10000):\n",
    "    l0 = x\n",
    "    l1 = nonlin(dot(l0, synaptic_weights_1))\n",
    "    l2 = nonlin(dot(l1, synaptic_weights_2))\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (i % 100) == 0:\n",
    "        print(\"Error: \"+ str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "    # This means less confident weights are adjusted more.\n",
    "    # This means inputs, which are zero, do not cause changes to the weights.\n",
    "    ###       Backpropogation       ###\n",
    "    l2_delta = l2_error * nonlin(l2, True)\n",
    "    l1_error = l2_delta.dot(synaptic_weights_2.T)\n",
    "    \n",
    "    l1_delta = l1_error * nonlin(dot(l1,True))\n",
    "    \n",
    "    synaptic_weights_2 += l1.T.dot(l2_delta)\n",
    "    synaptic_weights_1 += l0.T.dot(l1_delta)\n",
    "\n",
    "print(\"synaptic_weights_1: \", synaptic_weights_1)\n",
    "print(\"synaptic_weights_2: \", synaptic_weights_2)\n",
    "print(\"Output:\")\n",
    "print(l2)\n",
    "\n",
    "new_input = np.array([1, 0, 0])\n",
    "print(new_input.shape)\n",
    "dot_p = np.dot(new_input, synaptic_weights_2)\n",
    "print(nonlin(dot_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
